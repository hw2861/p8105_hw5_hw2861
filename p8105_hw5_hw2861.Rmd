---
title: "p8105_hw5_hw2861"
author: "Hongmiao Wang"
date: "2022-11-12"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 2
### Describe the raw data
```{r homicides_raw}
homicide_raw_df = 
  read_csv("./Data/homicide-data.csv")
```

This raw dataset has `r nrow(homicide_raw_df)` observations and `r ncol(homicide_raw_df)` variables. It collected data on `r nrow(homicide_raw_df)` criminal homicides over the past decade in 50 of the largest American cities. The data included the location of the killing, whether an arrest was made and, in most cases, basic demographic information about each victim.

**create Create a city_state variable**
```{r homicides}
homicide_df = 
  read_csv("./Data/homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = ","),
    city_state = recode(city_state,"Tulsa,AL" = "Tulsa,OK"),
    status = case_when(
      disposition == "Closed without arrest" | disposition == "Open/No arrest"   ~ "unsolved",
      TRUE ~ "solved"))
```

### Summarize within cities: to obtain the total number of homicides and the number of unsolved homicides

**The total number of homicides and the number of unsolved homicides in each cities.** 
```{r }
homicides_df_number= 
  homicide_df %>%
  select(city_state,status)%>%
  group_by(city_state)%>%
  summarize(
    total_homicides=n(),
    unsolved_homicides=sum(status=="unsolved")) 

    knitr::kable(homicides_df_number)
```

**The total number of homicides and the number of unsolved homicides in (e.g. “Baltimore, MD”)  cities.** 
```{r summary_baltimore}
homicides_baltimore= 
  homicide_df %>%
  select(city_state,status)%>%
  filter(city_state == "Baltimore,MD") %>%
  group_by(city_state)%>%
  summarize(
    total_homicides=n(),
    unsolved_homicides=sum(status=="unsolved")) 

    knitr::kable(homicides_baltimore)
```

**The total number of homicides in “Baltimore, MD” is 2827 and the number of unsolved homicides in “Baltimore, MD” is 1825.**

### For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved
```{r homicides_md_test}
Baltimore_prop=prop.test(
  x= homicides_df_number %>% filter(city_state == "Baltimore,MD")%>% pull(unsolved_homicides), 
  n= homicides_df_number %>% filter(city_state == "Baltimore,MD")%>% pull(total_homicides)
  ) %>% 
  broom::tidy()%>% 
  select(estimate,conf.low,conf.high)

Baltimore_prop

saveRDS(Baltimore_prop, file = "./result/Baltimore_prop.rds")
```
**The proportion of homicides that are unsolved in “Baltimore, MD”  is `r Baltimore_prop$estimate` . 
The 95CI of the proportion is between `r Baltimore_prop$conf.low` and  `r Baltimore_prop$conf.high`.**
I Saved the output to a RDS file.

### Now run prop.test for each of the cities in my dataset.

**Run the function for each of the cities**
```{r city_prop_df}
city_prop_df =
  homicides_df_number %>%
  mutate(
    tests_pre = map2(unsolved_homicides, total_homicides, prop.test),
    test = map(tests_pre, broom::tidy)
  ) %>%
  select(city_state,test)%>%
  unnest(test)%>%
  select(city_state, estimate, conf.low, conf.high)
```


### Create a plot that shows the estimates and 95CIs for each city
```{r city_prop_plot}
city_prop_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  labs(
    x = "City,State",
    y = "The proportion of homicides unsolved")+
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 80, hjust = 1))
```

## Problem 3
**In this problem, I will conduct a simulation to explore power in a one-sample t-test.**

### Set the following design elements and generate 5000 datasets from the model given.

**I create a function for the t-test**
```{r simulation}
ttest_function = function(n=30, mu, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma)
  )
  
  t_test = sim_data %>% 
    t.test() %>%
    broom::tidy() %>%
    select(estimate, p.value)
  
return(t_test)
}
```


**Generate 5000 datasets from the model and repeat the above for different mu（0 to 6)**
```{r }
sim_results_df = 
  tibble(mu = c(0,1,2,3,4,5,6)) %>% 
  mutate(
    output_lists = map(.x = mu, ~rerun(5000, ttest_function(mu = .x))),
    estimate_dfs = map(output_lists, bind_rows)) %>% 
  select(-output_lists) %>% 
  unnest(estimate_dfs)
```

### Make a plot showing the proportion of times the null was rejected
```{r test_plot}
sim_results_df %>%
  group_by(mu)%>%
  summarize(
    total=n(),
    rejected = sum(p.value<0.05),
    prop = rejected/ total) %>% 
  ggplot(aes(x = mu, y = prop)) +
  geom_point() + 
  geom_line() +
  labs(
    x = "μ value",
    y = "The proportion of times the null was rejected",
    title = "Association between effect size and power")+
  theme(plot.title = element_text(hjust = 0.5),axis.text = element_text(size = 13))
```

**Describe the association between effect size and power**:
* At μ value < 4, power increases as the effect size increases. 
* At μ value > 4, the power tends to stabilize at 1.


### Make 2 plots about the average estimate of μ^ 
The first plot: showing the average estimate of μ^ on the y axis and the true value of μ on the x axis. 
The second plot: showing the average estimate of μ^ only in samples for which the null was rejected on the y axis and the true value of μ on the x axis.
```{r }
plot1=
  sim_results_df %>%
  group_by(mu)%>%
   summarize(
    total=n(),
    mu_hat=mean(estimate))%>%
    mutate(type = "Total sample")



plot2= 
  sim_results_df %>%
  filter(p.value<0.05)%>%
  group_by(mu)%>%
   summarize(
    total=n(),
    mu_hat=mean(estimate))%>%
    mutate(type = "Samples for which the null was rejected")
    
    
combine = full_join(plot1, plot2)%>%
   ggplot(aes(y = mu_hat, x = mu,color=type))+
    geom_line(size=1.3) +
    geom_point(size=2)+
    labs(
    x = "The true value of μ",
    y = "The average estimate of μ^",
    title = "Average estimate mean vs The true mean")+
  theme(plot.title = element_text(hjust = 0.5),axis.text = element_text(size = 13),legend.position = "bottom",)

combine
```

For which the null is rejected,
When the true μ >4, the sample average estimate of μ^ is approximately equal to the true value. 
when the true μ value is between 0 and 4(especially if true value = 1):They have relatively large differences.


When the true μ >4, the effect size is large enough and the proportion of times the null was rejected is nearly 1. At this point, the power is strong and almost all samples are included and consider as rejected null. The average estimated mean calculated from all 5000 datasets is close to the true mean.


When the true 0 <μ <4, the effect size is Not large enough and the true μ is relatively close to 0. The proportion of times the null was rejected is low. At this point, many samples were screened out because they did not reject Null. Only a few number of datasets are included in our analysis (perhaps even just a few hundred).Then, average estimated mean is not an good representation of true value.

